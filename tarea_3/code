import breeze.linalg.{Vector, DenseVector}
import org.apache.Spark._

def main(args: Array[String]) {
	val sparkConf = new SparkConf().setAppName("Spark kMeans")
	val sc = new SparkContext(sparkConf)
	val lines = sc.textFile(args[0])
	val data = lines.map(parseVector_).cache()
	val k = args[1].toInt // K prototipos
	val convergeDist = args[2].toDouble
	val kPoints = data.takeSample(withReplacement=false, k, 50).toArray
	val tempDist = 1.0	
	while (tempDist > convergeDist) {
		val closest = data.map(p => (closestPoint(p, kPoints), (p, 1))) // closestPoint id_centroide | (p, 1) el 1 es para conteo
		val pointStats = closest.reduceByKey{ case ((p1, c2), (p2, c2)) => (p1 + p2, c1 + c2) } // <c_id, [(p1, 1), (p2, 1), ...]>
		val newPoints = pointStats.map{ pair =>
			(pair._1, pair._2._1 * (1.0 / pair._2._2)) // pair._1 indice, pair._2._1  \sum_{p\in Centroide} p_i ._1 es el valor | pair._2._2 numero de puntos de cluster
		}.collectAsMap()
		for (i <- 0 until k) tempDist += euclidean(kPoints(i), newPoints(i))
		for (newP <- newPoints) kPoints(newP._1) = newP._2
	}
}